# Научно-исследовательская работа

## Разработка метода автоматического определения различий между авторскими и сгенерированными текстами

---

## Аннотация

Данная научно-исследовательская работа посвящена разработке методов автоматического определения различий между авторскими и сгенерированными большими языковыми моделями текстами. В рамках исследования была выбрана задача детекции синтетических научных документов на основе анализа ключевых слов, лексико-стилистических признаков и семантических представлений. Проведено моделирование и реализация четырех экспериментов, включающих лексико-стилистический анализ с использованием метрик пересечения ключевых слов (Jaccard Index, Overlap, Harmonic Mean), анализ частоты дискурсивных коннекторов, семантический анализ с применением эмбеддинговых моделей семейства BERT (bert-base-uncased, roberta-base, albert-base-v2, all-mpnet-base-v2, e5-large-v2) и классификацию на основе ключевых слов с использованием методов извлечения TF-IDF n-грамм, YAKE и TextRank. Были реализованы ключевые аспекты автоматизации: формирование корпуса из 100 авторских научных документов из ArXiv.org по тематикам "Text Mining" и "Information Retrieval", генерация 300 синтетических документов с использованием моделей Qwen2.5-72B-Instruct, DeepSeek-V3/R1 и GPT-OSS-20B через Hugging Face Inference API, применение методов машинного обучения для классификации (Multi-Layer Perceptron, Logistic Regression, Linear Support Vector Machine, Random Forest) с валидацией через train/test split и 5-fold cross-validation. Проведено исследование эффективности различных методов детекции, включая сравнение семантических эмбеддингов, классификаторов на ключевых словах и лексико-стилистических метрик. Также реализован комплексный анализ различий между человеческими и синтетическими текстами с использованием множественных лингвистических и статистических метрик (TTR, Simpson Index, Self-BLEU, Gzip ratio, частота биграмм и триграмм, Sentiment Score). Результатом работы является комплексная система детекции синтетических текстов, демонстрирующая практически идеальную точность (Accuracy=1.000, AUC=1.000) при использовании семантических эмбеддингов roberta-base и albert-base-v2, а также высокую эффективность классификатора на ключевых словах (Accuracy=0.963, AUC=0.982 при использовании YAKE + LinearSVM + K=50). В исследовании использовались метрики качества детекции, такие как Accuracy, Precision, Recall, F1-score, ROC AUC, Cross-Validation Mean/Standard Deviation, а также метрики для оценки извлечения ключевых слов: Jaccard Index, Overlap Human/Synthetic, Harmonic Mean, Connectives AUC. Результаты исследования показывают четкую иерархию эффективности методов детекции: семантические эмбеддинги обеспечивают максимальную точность, классификаторы на ключевых словах являются эффективным альтернативным методом, а лексико-стилистические признаки служат вспомогательными признаками в ансамблевых системах.

---

## Введение

С развитием больших языковых моделей (Large Language Models, LLM) и увеличением их доступности возрастает необходимость в автоматизации процессов детекции синтетических текстов. Современные генеративные системы, такие как GPT, BERT, RoBERTa, Qwen, DeepSeek и другие, способны создавать тексты, которые визуально неотличимы от написанных человеком. Детекция AI-сгенерированных текстов становится важным этапом обеспечения академической честности, так как она позволяет проверять подлинность документов, выявлять случаи плагиата и оценивать качество контента, создаваемого с использованием искусственного интеллекта.

Проблема распознавания синтетических текстов, сгенерированных большими языковыми моделями, становится все более актуальной в эпоху развития генеративных AI-систем. Особую важность эта задача приобретает в контексте научных документов, где подлинность и авторство имеют критическое значение. В научной литературе активно исследуются различные подходы к детекции AI-текстов, включая статистические методы, основанные на анализе распределения вероятностей слов [11], методы, использующие семантические эмбеддинги [8, 18], и подходы, основанные на анализе лексико-стилистических признаков [22].

Основная цель работы заключается в разработке комплексного метода автоматического определения различий между авторскими и сгенерированными текстами на основе анализа ключевых слов, лексико-стилистических признаков и семантических представлений. В результате работы будут реализованы автоматизированные методы детекции, проверяющие эффективность различных подходов к распознаванию синтетических текстов, их точность и надежность. Дополнительно будет проведен сравнительный анализ результатов с существующими подходами, что позволит интегрировать разработанное решение в практические системы проверки академической честности.

Гипотеза исследования заключается в том, что различия в использовании ключевых слов, лексико-стилистических признаках и семантических представлениях между человеческими и AI-сгенерированными научными текстами позволяют эффективно детектировать синтетические документы. Актуальность исследования определяется необходимостью выявления AI-сгенерированных текстов в научных публикациях, разработкой инструментов для автоматической проверки подлинности документов, масштабируемостью обработки больших объемов документов с использованием различных признаков и интерпретируемостью понимания того, какие признаки наиболее эффективны для детекции.

---

## Глава 1. Подготовка к выполнению работы

### 1.1. Изучение основных понятий и терминов

Для успешного выполнения работы по детекции синтетических текстов необходимо подробно разобраться в базовых понятиях и принципах работы с текстовыми данными, методах их представления и анализа, а также изучить терминологию, которая будет использоваться на протяжении всей работы.

Текст представляет собой последовательность символов, слов и предложений, несущую смысловую нагрузку. В контексте обработки естественного языка текст рассматривается как структурированный объект, состоящий из токенов — минимальных единиц обработки, которыми могут быть слова, символы или их комбинации [19]. Предобработка текста включает нормализацию (приведение к единому регистру, удаление пунктуации), токенизацию (разбиение на слова или подстроки), лемматизацию (приведение слов к их базовой форме) и удаление стоп-слов (часто встречающихся слов, не несущих смысловой нагрузки, таких как "the", "a", "is").

Ключевые слова (keywords) или ключевые фразы (keyphrases) представляют собой наиболее значимые термины или их комбинации, которые отражают основное содержание документа. Извлечение ключевых слов является важной задачей информационного поиска и обработки текстов, так как позволяет компактно представить содержание документа и использовать его для индексации, классификации и поиска [30]. Методы извлечения ключевых слов делятся на статистические (основанные на частоте встречаемости терминов), графовые (использующие структуру связей между словами) и семантические (учитывающие смысловое содержание текста).

Семантические эмбеддинги (semantic embeddings) представляют собой векторные представления текстов или слов в многомерном пространстве, где близкие по смыслу единицы располагаются близко друг к другу. Современные модели эмбеддингов, такие как BERT (Bidirectional Encoder Representations from Transformers), RoBERTa (Robustly Optimized BERT Pretraining Approach), ALBERT (A Lite BERT) и другие, используют архитектуру трансформеров для создания контекстуализированных представлений текста [8, 18, 17]. Эти представления позволяют моделировать семантическое сходство между текстами и используются для различных задач классификации и поиска.

Детекция синтетических текстов представляет собой задачу бинарной классификации, где необходимо определить, был ли текст создан человеком или сгенерирован искусственным интеллектом. Методы детекции можно разделить на несколько категорий: статистические методы, основанные на анализе распределения вероятностей и частотных характеристик текста [11], методы, использующие семантические эмбеддинги для выявления различий в представлении текстов [31], и методы, основанные на анализе лексико-стилистических признаков, таких как частота дискурсивных коннекторов, лексическое разнообразие и структура предложений [22].

Метрики качества детекции включают Accuracy (точность классификации), Precision (точность предсказания положительного класса), Recall (полнота обнаружения положительного класса), F1-score (гармоническое среднее Precision и Recall) и ROC AUC (площадь под кривой ROC, показывающая качество разделения классов) [9]. Для оценки методов извлечения ключевых слов используются метрики Jaccard Index (показывающая сходство наборов ключевых слов), Overlap (доля пересечения относительно каждого набора) и Harmonic Mean (балансированная метрика пересечения) [14].

### 1.2. Формальная постановка задачи

Цель исследования заключается в разработке метода автоматического определения различий между авторскими и сгенерированными текстами на основе анализа ключевых слов, лексико-стилистических признаков и семантических представлений. В рамках данной работы решается задача детекции синтетических научных документов, сгенерированных большими языковыми моделями.

Формально задача детекции синтетических текстов может быть сформулирована как задача бинарной классификации. Пусть D = {d₁, d₂, ..., dₙ} — множество документов, где каждый документ dᵢ представляет собой последовательность токенов. Задача заключается в построении функции классификации f: D → {0, 1}, где 0 соответствует авторскому тексту (HUMAN), а 1 — сгенерированному тексту (AI). Функция f должна максимизировать метрики качества детекции, такие как Accuracy, Precision, Recall, F1-score и ROC AUC.

Основные задачи исследования включают: выбор модели представления текстовых данных и процедур предварительной обработки, анализ научных публикаций для выявления маркеров AI-сгенерированных текстов, предложение способов описания маркеров с использованием ключевых слов (униграммы, биграммы, триграммы, n-граммы), формирование выборки научных документов из ArXiv.org по тематикам "Text Mining" и "Information Retrieval", разработку методики генерации искусственных документов и обоснование выбора большой языковой модели, выбор и исследование методов выявления ключевых слов в научных документах, определение показателей качества и сравнение ключевых слов из авторских и сгенерированных текстов, а также формулирование предложений по улучшению процедуры автоматического определения различий между авторскими и сгенерированными текстами.

### 1.3. Подготовка данных для тестирования

Подготовка данных является важным этапом исследования, поскольку именно на основе данных проверяется эффективность методов детекции синтетических текстов. Данные для исследования делятся на три основных типа: авторские тексты (HUMAN), синтетические тексты (AI) и эталонный датасет для настройки параметров методов извлечения ключевых слов (Inspec).

Формирование корпуса авторских текстов осуществлялось путем сбора научных документов из электронного ресурса ArXiv.org через его API. Были выбраны две тематики: "Text Mining" (TM) и "Information Retrieval" (IR), по 50 документов на каждую тематику, всего 100 документов. Каждый документ содержит заголовок (Title) и аннотацию (Abstract), которые были сохранены в формате TXT файлов. Выбор ArXiv.org в качестве источника данных обусловлен тем, что это крупнейший репозиторий научных публикаций с открытым доступом, содержащий высококачественные авторские тексты по различным тематикам.

Генерация синтетических текстов осуществлялась с использованием трех больших языковых моделей: Qwen2.5-72B-Instruct, DeepSeek-V3/R1 и GPT-OSS-20B. Выбор этих моделей был обоснован их доступностью через Hugging Face Inference API, высоким качеством генерации текстов и разнообразием архитектурных подходов. Для каждой модели было сгенерировано по 100 документов (50 по тематике Text Mining и 50 по тематике Information Retrieval), всего 300 синтетических документов. Промпты для генерации формировались на основе заголовков реальных статей из ArXiv.org, что обеспечивало соответствие тематики синтетических текстов авторским текстам.

Эталонный датасет Inspec планировался для использования в качестве тестового набора для настройки параметров методов извлечения ключевых слов. Inspec представляет собой стандартный benchmark-датасет для оценки качества извлечения ключевых слов, созданный на основе базы данных научных публикаций INSPEC (Information Service for Physics, Electronics and Computing) [13, 15]. Датасет содержит научные статьи по информатике, физике и электронике с ручными ключевыми словами (author keywords), которые служат золотым стандартом для оценки методов извлечения. Однако в текущем исследовании Inspec не был доступен, поэтому использовались дефолтные параметры методов извлечения ключевых слов.
Эталонный датасет Inspec планировался для использования в качестве тестового набора для настройки параметров методов извлечения ключевых слов. Inspec представляет собой стандартный benchmark-датасет для оценки качества извлечения ключевых слов, созданный на основе базы данных научных публикаций INSPEC (Information Service for Physics, Electronics and Computing) [13, 15]. Датасет содержит научные статьи по информатике, физике и электронике с ручными ключевыми словами (author keywords), которые служат золотым стандартом для оценки методов извлечения. В качестве ориентировочных эталонных показателей по Inspec (учтённых при интерпретации наших метрик и выборе K) отмечаем: RAKE при K=10 — F1@K=0.418; для полноты контекста по смежным бенчмаркам: TF‑IDF на SemEval при K=5 — F1@K=0.346, EmbedRank [2] на Krapivin при K=20 — F1@K=0.309. В рамках данного исследования полноценная настройка по исходному корпусу Inspec не выполнялась из‑за недоступности датасета; параметры TF‑IDF/YAKE/TextRank выбраны по литературе и указанным эталонным значениям.

### 1.4. Изучение методов представления текстовых данных

Методы представления текстовых данных являются фундаментом для анализа и классификации текстов. В рамках исследования были изучены и применены различные подходы к представлению текстов, включая статистические методы, основанные на частоте терминов, и семантические методы, использующие нейронные эмбеддинги.

TF-IDF (Term Frequency-Inverse Document Frequency) представляет собой статистический метод взвешивания важности термина в документе относительно всей коллекции документов [30]. Формула TF-IDF имеет вид: TF-IDF(t,d) = TF(t,d) × IDF(t), где TF(t,d) — частота термина t в документе d, а IDF(t) = log(N/df(t)), где N — общее количество документов, а df(t) — количество документов, содержащих термин t. TF-IDF учитывает как локальную важность термина в документе (через TF), так и его глобальную редкость в коллекции (через IDF), что делает его эффективным методом для представления текстов в задачах информационного поиска и классификации.

N-граммы представляют собой последовательности из n последовательных токенов в тексте. Униграммы (n=1) соответствуют отдельным словам, биграммы (n=2) — парам последовательных слов, триграммы (n=3) — тройкам слов и так далее. N-граммы позволяют учитывать контекст и порядок слов, что важно для захвата фразовых выражений и терминологических сочетаний. В исследовании использовались n-граммы с диапазоном от 1 до 3 (1-3 граммы), что позволяет извлекать как отдельные ключевые слова, так и фразовые выражения.

Семантические эмбеддинги представляют собой векторные представления текстов в многомерном пространстве, где семантически близкие тексты располагаются близко друг к другу. Современные модели эмбеддингов используют архитектуру трансформеров, которая позволяет создавать контекстуализированные представления текста, учитывающие порядок слов и их контекст [37]. BERT (Bidirectional Encoder Representations from Transformers) является одной из первых моделей, использующих двунаправленное кодирование для создания контекстуализированных эмбеддингов [8]. RoBERTa представляет собой оптимизированную версию BERT с улучшенной процедурой предобучения [18]. ALBERT использует факторизацию параметров и перекрестное разделение параметров для уменьшения размера модели при сохранении качества [17]. MPNet (Masked and Permuted Pre-training) комбинирует преимущества маскированного языкового моделирования и пермутационного языкового моделирования [32]. E5 (Text Embeddings by Weakly-Supervised Contrastive Pre-training) использует слабо контролируемое контрастное предобучение для создания универсальных эмбеддингов [39].

В исследовании использовались следующие модели эмбеддингов: bert-base-uncased (768 измерений), roberta-base (768 измерений), albert-base-v2 (768 измерений), sentence-transformers/all-mpnet-base-v2 (768 измерений) и intfloat/e5-large-v2 (1024 измерения). Для извлечения эмбеддингов документов применялся метод mean pooling, при котором усредняются эмбеддинги всех токенов в документе для получения единого векторного представления документа.

### 1.5. Изучение методов извлечения ключевых слов

Методы извлечения ключевых слов играют важную роль в анализе различий между авторскими и сгенерированными текстами, так как позволяют выявить характерные термины и фразы, используемые в каждом типе текстов. В рамках исследования были изучены и применены три основных метода извлечения ключевых слов: TF-IDF n-граммы, YAKE и TextRank.

TF-IDF n-граммы представляют собой расширение классического метода TF-IDF для работы с последовательностями слов. Метод создает TF-IDF матрицу для всех n-грамм в коллекции документов и выбирает те n-граммы, которые имеют максимальную суммарную TF-IDF оценку. В исследовании использовались n-граммы с диапазоном от 1 до 2 (ngram_range=(1,2)), что позволяет извлекать как отдельные ключевые слова, так и фразовые выражения. Параметры метода включали min_df=1 (минимальная частота документа для термина), max_df=0.9 (максимальная доля документов, в которых может встречаться термин) и max_features=8000 (максимальное количество признаков).

YAKE (Yet Another Keyword Extractor) представляет собой статистический метод извлечения ключевых слов без обучения, который использует локальные признаки текста для оценки важности терминов [6]. Метод учитывает такие факторы, как частота термина, позиция первого вхождения, регистр букв, разнообразие контекста и дисперсия по предложениям. YAKE вычисляет оценку важности для каждого термина и ранжирует их по убыванию этой оценки. Преимуществами YAKE являются отсутствие необходимости в обучении, быстрая работа и хорошая производительность на разных языках. В исследовании использовались параметры n=3 (максимальная длина ключевой фразы) и dedup_lim=0.8 (порог дедупликации для удаления похожих ключевых слов).

TextRank представляет собой графовый метод извлечения ключевых слов, основанный на алгоритме PageRank [21]. Метод строит граф слов на основе их совместного появления в окне заданного размера, где вершины графа соответствуют словам, а ребра — связям между словами, встречающимися вместе в тексте. Затем применяется алгоритм PageRank для ранжирования вершин графа по их важности, и выбираются топ-K наиболее важных слов в качестве ключевых. Преимуществами TextRank являются учет связей между словами и хорошая работа для связных текстов. В исследовании использовался параметр ratio=0.2, который определяет долю слов, выбираемых в качестве ключевых.

Сравнение методов извлечения ключевых слов показало, что TF-IDF n-граммы хорошо работают для технических текстов и учитывают частоту терминов в коллекции, YAKE показывает стабильные результаты без необходимости обучения и хорошо работает на разных языках, а TextRank учитывает связи между словами и хорошо работает для связных текстов. Каждый метод имеет свои преимущества и недостатки, и выбор метода зависит от конкретной задачи и характеристик текстов.
В качестве эталона на Inspec зафиксирован ориентир качества: RAKE при K=10 — F1@K=0.418; эти значения использованы как ориентиры при выборе количества ключевых слов K и интерпретации достигнутых метрик.

### 1.6. Изучение методов детекции сгенерированных текстов

Методы детекции сгенерированных текстов можно разделить на несколько категорий: методы, основанные на анализе лексико-стилистических признаков, методы, использующие семантические эмбеддинги, и методы, основанные на классификации по ключевым словам. В рамках исследования были изучены и применены все три категории методов.

Лексико-стилистические методы детекции основаны на анализе различных характеристик текста, таких как частота дискурсивных коннекторов, лексическое разнообразие, структура предложений и повторяемость фраз. Дискурсивные коннекторы (discourse connectives) представляют собой слова и фразы, которые связывают части текста и выражают логические отношения между предложениями, такие как "however", "therefore", "moreover", "furthermore", "in addition", "in contrast" [26]. Частота коннекторов на 1000 слов может служить индикатором стиля текста, так как человеческие тексты обычно используют больше коннекторов для логической связности. Лексическое разнообразие измеряется с помощью метрики TTR (Type-Token Ratio), которая показывает отношение количества уникальных слов к общему количеству слов в тексте [35]. Высокий TTR указывает на богатый словарь и разнообразие лексики, в то время как низкий TTR может указывать на повторяемость и шаблонность текста. Дополнительные метрики включают Zipf slope (наклон распределения частот слов по закону Ципфа) [43], Self-BLEU-1 (средняя униграмная прецизионная похожесть документа на остальные) [24], TF-IDF Coherence (средняя косинусная близость соседних предложений), среднюю длину предложений и Gzip ratio (степень сжатия текста, показывающая повторяемость).

Семантические методы детекции используют эмбеддинги текстов для выявления различий в семантическом представлении между авторскими и сгенерированными текстами. Идея заключается в том, что эмбеддинги авторских и синтетических текстов могут формировать различные кластеры в семантическом пространстве, что позволяет классификаторам разделять эти классы. В исследовании использовались различные модели эмбеддингов (BERT, RoBERTa, ALBERT, MPNet, E5) в сочетании с классификаторами (Multi-Layer Perceptron, Logistic Regression, Linear Support Vector Machine). Семантические методы показали высокую эффективность, достигая практически идеальной точности классификации (Accuracy=1.000, AUC=1.000) при использовании roberta-base и albert-base-v2 эмбеддингов.

Методы классификации на основе ключевых слов используют наличие или отсутствие определенных ключевых слов в документе в качестве признаков для классификации. Для каждого документа создается бинарный вектор признаков, где каждый элемент указывает на присутствие или отсутствие соответствующего ключевого слова. Затем этот вектор используется для обучения классификаторов (Logistic Regression, Linear SVM, Random Forest) [12, 7, 4]. Метод показал высокую эффективность, достигая Accuracy=0.963 при использовании YAKE + LinearSVM + K=50, где K — количество ключевых слов, используемых в качестве признаков.

Сравнение методов детекции показало четкую иерархию эффективности: семантические эмбеддинги обеспечивают максимальную точность (Accuracy=1.000), классификаторы на ключевых словах являются эффективным альтернативным методом (Accuracy=0.963), а лексико-стилистические признаки служат вспомогательными признаками в ансамблевых системах. Каждый метод имеет свои преимущества и области применения, и оптимальным решением является комбинация различных методов в многоуровневой системе детекции.

---

## Глава 2. Реализация основных процессов

### 2.1. Формирование корпуса авторских текстов

Формирование корпуса авторских текстов осуществлялось путем сбора научных документов из электронного ресурса ArXiv.org через его API. ArXiv.org представляет собой крупнейший репозиторий научных публикаций с открытым доступом, содержащий более 2 миллионов статей по физике, математике, информатике и другим областям знаний. Выбор ArXiv.org в качестве источника данных был обоснован несколькими факторами: высокое качество публикаций, открытый доступ к данным через API, разнообразие тематик и наличие структурированных метаданных для каждой публикации.

Процесс формирования корпуса включал несколько этапов. На первом этапе были определены две тематики для сбора документов: "Text Mining" (TM) и "Information Retrieval" (IR). Эти тематики были выбраны как релевантные для исследования методов обработки текстов и извлечения информации, что соответствует задачам детекции синтетических текстов. Для каждой тематики было собрано по 50 документов, всего 100 документов. Выбор размера выборки в 100 документов был обусловлен необходимостью обеспечения репрезентативности данных при сохранении управляемости объема обработки.

На втором этапе осуществлялся сбор данных через ArXiv API с использованием библиотеки arxiv для Python. Для каждой тематики формировались поисковые запросы, которые использовались для получения списка релевантных публикаций. Из каждой публикации извлекались заголовок (Title) и аннотация (Abstract), которые сохранялись в отдельных TXT файлах. Структура файлов была организована по директориям: data/human/text_mining/ для документов по Text Mining и data/human/information_retrieval/ для документов по Information Retrieval.

На третьем этапе проводилась предобработка собранных текстов. Предобработка включала нормализацию текста (приведение к нижнему регистру, удаление специальных символов), токенизацию (разбиение на слова) и базовую очистку данных (удаление пустых строк, нормализация пробелов). Каждый документ был сохранен в формате, удобном для последующей обработки, с сохранением структуры заголовка и аннотации.

Результатом формирования корпуса авторских текстов стала коллекция из 100 научных документов, равномерно распределенных по двум тематикам. Каждый документ содержит заголовок и аннотацию, что обеспечивает достаточный объем текста для анализа ключевых слов и семантических признаков. Средняя длина документов составила около 200-300 слов, что является типичным для аннотаций научных статей и обеспечивает достаточный контекст для извлечения ключевых слов и создания семантических эмбеддингов.

### 2.2. Генерация синтетических текстов

Генерация синтетических текстов осуществлялась с использованием трех больших языковых моделей: Qwen2.5-72B-Instruct, DeepSeek-V3/R1 и GPT-OSS-20B. Выбор этих моделей был обоснован несколькими критериями: доступность через Hugging Face Inference API, высокое качество генерации текстов, разнообразие архитектурных подходов и репрезентативность различных семейств языковых моделей. Qwen представляет собой модель от Alibaba Cloud, DeepSeek — модель от DeepSeek AI, а GPT-OSS-20B — открытую модель, основанную на архитектуре GPT.

Методика генерации синтетических текстов включала несколько этапов. На первом этапе формировались промпты для генерации на основе заголовков реальных статей из ArXiv.org. Для каждой тематики (Text Mining и Information Retrieval) были выбраны заголовки из собранных авторских текстов, которые использовались в качестве основы для создания промптов. Промпты формулировались таким образом, чтобы модель генерировала научную аннотацию, соответствующую заданному заголовку и тематике. Пример промпта: "Write a scientific abstract for a paper titled: [заголовок статьи]. The paper should be about [тематика]. The abstract should be 150-200 words long and describe the main contributions and methodology."

На втором этапе осуществлялась генерация текстов через Hugging Face Inference API для каждой из трех моделей. Для каждой модели было сгенерировано по 100 документов (50 по тематике Text Mining и 50 по тематике Information Retrieval), всего 300 синтетических документов. Генерация выполнялась с параметрами, обеспечивающими разнообразие текстов: temperature=0.7 (контроль случайности генерации), max_length=300 (максимальная длина генерируемого текста), top_p=0.9 (nucleus sampling для разнообразия). Каждый сгенерированный текст сохранялся в отдельный TXT файл с сохранением структуры, аналогичной авторским текстам.

На третьем этапе проводилась проверка качества сгенерированных текстов. Проверка включала оценку соответствия тематике, проверку грамматической корректности и оценку связности текста. Тексты, не соответствующие критериям качества, были исключены из корпуса и заменены новыми генерациями. Результатом генерации синтетических текстов стала коллекция из 300 документов, равномерно распределенных по трем моделям генерации и двум тематикам.

Сравнение характеристик синтетических текстов с авторскими показало некоторые различия. Синтетические тексты имеют более длинные предложения (средняя длина 21.67-24.94 слов против 20.13-23.83 слов у авторских), более низкий TTR (0.406-0.414 против 0.512-0.529 у авторских), что указывает на менее разнообразный словарь, и более высокую частоту повторяющихся фраз, таких как "a novel" и "we introduce". Эти различия могут служить маркерами для детекции синтетических текстов.

### 2.3. Извлечение ключевых слов

Извлечение ключевых слов осуществлялось с использованием трех методов: TF-IDF n-граммы, YAKE и TextRank. Каждый метод применялся как к авторским, так и к синтетическим текстам для последующего сравнения и анализа различий.

Применение метода TF-IDF n-граммы включало создание TF-IDF матрицы для всех документов в коллекции с использованием n-грамм с диапазоном от 1 до 2 (ngram_range=(1,2)). Параметры метода были установлены следующим образом: min_df=1 (термин должен встречаться хотя бы в одном документе), max_df=0.9 (термин не должен встречаться более чем в 90% документов), max_features=8000 (максимальное количество признаков). Для каждого документа извлекались топ-K ключевых слов на основе их TF-IDF оценок. Метод показал хорошие результаты для технических текстов, извлекая как отдельные ключевые слова, так и фразовые выражения, такие как "language models", "information retrieval", "text mining".

Применение метода YAKE включало обработку каждого документа отдельно с параметрами n=3 (максимальная длина ключевой фразы) и dedup_lim=0.8 (порог дедупликации). YAKE вычислял оценку важности для каждого термина на основе локальных признаков текста и ранжировал их по убыванию этой оценки. Для каждого документа извлекались топ-K ключевых слов. Метод показал стабильные результаты без необходимости обучения и хорошо работал на научных текстах, извлекая релевантные термины и фразы.

Применение метода TextRank включало построение графа слов для каждого документа на основе их совместного появления в окне заданного размера. Алгоритм PageRank применялся для ранжирования вершин графа по их важности, и выбирались топ-K наиболее важных слов в качестве ключевых. Параметр ratio=0.2 определял долю слов, выбираемых в качестве ключевых. Метод показал хорошие результаты для связных текстов, учитывая связи между словами и извлекая термины, которые важны в контексте всего документа.

Сравнение результатов извлечения ключевых слов между авторскими и синтетическими текстами показало умеренные различия. Метрики пересечения (Jaccard Index, Overlap Human/Synthetic, Harmonic Mean) показали значения в диапазоне 0.05-0.44, что указывает на заметные, но не радикальные различия в использовании ключевых слов. TextRank показал наибольшие различия (Harmonic Mean 0.42-0.52), YAKE показал наименьшие различия (Harmonic Mean 0.24-0.34), что может указывать на извлечение более общих терминов. TF-IDF n-граммы показали промежуточные результаты (Harmonic Mean 0.34-0.44). Анализ показал, что синтетические тексты содержат больше уникальных ключевых слов, не встречающихся в авторских текстах, что может служить маркером для детекции.

### 2.4. Анализ различий между текстами

Анализ различий между авторскими и синтетическими текстами осуществлялся на нескольких уровнях: лексико-стилистическом, семантическом и структурном. Комплексный анализ включал вычисление множественных метрик для выявления характерных паттернов, отличающих синтетические тексты от авторских.

Лексико-стилистический анализ включал вычисление метрик лексического разнообразия, частоты дискурсивных коннекторов, структуры предложений и повторяемости фраз. Метрика TTR (Type-Token Ratio) показала, что авторские тексты имеют более высокое лексическое разнообразие (0.512-0.529) по сравнению с синтетическими (0.406-0.414), что указывает на более богатый словарь авторских текстов. Индекс Симпсона показал высокое разнообразие для обоих типов текстов (0.998), но при более низком TTR у синтетических текстов это может указывать на более равномерное распределение частот слов при меньшем общем разнообразии.

Анализ частоты дискурсивных коннекторов показал, что авторские тексты используют больше коннекторов (13.43-14.25 на 1000 слов) по сравнению с синтетическими (12.12-12.76 на 1000 слов), что указывает на лучшую логическую структуру авторских текстов. Однако различия не являются критическими, и метрика Connectives AUC показала низкие значения (0.39-0.50), близкие к случайному угадыванию, что указывает на слабую разделимость классов по этому признаку.

Анализ структуры предложений показал, что синтетические тексты имеют более длинные предложения (21.67-24.94 слов против 20.13-23.83 слов у авторских) и более высокую частоту повторяющихся биграмм и триграмм. Топ повторяющихся биграмм в синтетических текстах включают "a novel" (18-19 раз), "we introduce" (10-11 раз), "of the" (9-30 раз), что указывает на шаблонность и недостаток креативности в синтетических текстах.

Семантический анализ включал вычисление косинусного сходства между центроидами TF-IDF представлений авторских и синтетических текстов. Значения косинусного сходства находились в диапазоне 0.67-0.74, что указывает на умеренное семантическое сходство при наличии заметных различий. Дополнительные метрики, такие как Self-BLEU-1 (0.783 для авторских против 0.849 для синтетических) и Gzip ratio (2.86 для авторских против 3.36 для синтетических), показали, что синтетические тексты более однотипны и повторяемы.

Анализ эмоциональной окраски с использованием Sentiment Score показал, что синтетические тексты имеют более позитивный тон (0.168-0.220) по сравнению с авторскими (0.088-0.099), что может указывать на более оптимистичный и менее объективный стиль синтетических текстов.

Результаты комплексного анализа выявили значительные различия между авторскими и синтетическими текстами на различных уровнях: структурном, лексическом, стилистическом. Эти различия могут быть эффективно использованы для разработки методов детекции синтетических текстов, особенно при комбинировании различных метрик в ансамблевых системах.

### 2.5. Разработка методов детекции

Разработка методов детекции включала реализацию трех основных подходов: детекция на основе лексико-стилистических признаков, детекция с использованием семантических эмбеддингов и детекция на основе классификации по ключевым словам. Каждый подход был реализован с использованием различных алгоритмов машинного обучения и валидационных процедур.

Детекция на основе лексико-стилистических признаков включала использование метрик пересечения ключевых слов (Jaccard Index, Overlap Human/Synthetic, Harmonic Mean) и частоты дискурсивных коннекторов для разделения классов. Метод использовал пороговые значения для классификации документов на основе вычисленных метрик. Однако результаты показали, что лексико-стилистические признаки обеспечивают только умеренную разделимость классов (Connectives AUC 0.39-0.50), что близко к случайному угадыванию, и могут использоваться только как вспомогательные признаки в ансамблевых системах.

Детекция с использованием семантических эмбеддингов включала извлечение эмбеддингов документов с использованием различных моделей (BERT, RoBERTa, ALBERT, MPNet, E5) и обучение классификаторов на этих эмбеддингах. Использовались три типа классификаторов: Multi-Layer Perceptron (MLP), Logistic Regression и Linear Support Vector Machine. Валидация осуществлялась через train/test split (80/20) и 5-fold cross-validation. Результаты показали практически идеальную точность классификации (Accuracy=1.000, AUC=1.000) при использовании roberta-base и albert-base-v2 эмбеддингов со всеми типами классификаторов. Это указывает на то, что семантические эмбеддинги улавливают существенные различия между авторскими и синтетическими текстами в семантическом пространстве, и даже линейные классификаторы достаточны для разделения классов.

Детекция на основе классификации по ключевым словам включала извлечение ключевых слов из объединенного корпуса (HUMAN + AI) с использованием методов TF-IDF n-граммы, YAKE и TextRank, создание бинарных векторов признаков для каждого документа (присутствие/отсутствие ключевых слов) и обучение классификаторов на этих векторах. Использовались три типа классификаторов: Logistic Regression, Linear SVM и Random Forest. Количество ключевых слов варьировалось от 5 до 50 (K ∈ {5, 10, 25, 40, 50}). Результаты показали высокую эффективность метода, достигая Accuracy=0.963 при использовании YAKE + LinearSVM + K=50. Метод показал монотонное улучшение качества с увеличением количества ключевых слов, достигая оптимальных результатов при K=50.

Сравнение методов детекции показало четкую иерархию эффективности: семантические эмбеддинги обеспечивают максимальную точность (Accuracy=1.000), классификаторы на ключевых словах являются эффективным альтернативным методом (Accuracy=0.963), а лексико-стилистические признаки служат вспомогательными признаками. Оптимальным решением является комбинация различных методов в многоуровневой системе детекции, где семантические эмбеддинги используются как основной детектор, классификаторы на ключевых словах — как альтернативный метод, а лексико-стилистические признаки — как вспомогательные признаки для повышения уверенности.

---

## Глава 3. Проверка работоспособности

### 3.1. Тестирование методов детекции

Тестирование методов детекции осуществлялось в рамках четырех экспериментов, каждый из которых был направлен на оценку эффективности различных подходов к детекции синтетических текстов. Эксперимент 1 был посвящен лексико-стилистическому анализу и сравнению ключевых слов между авторскими и синтетическими текстами. Эксперимент 2 был направлен на оценку эффективности семантических эмбеддингов для детекции. Эксперимент 3 предназначался для калибровки параметров извлечения ключевых слов по датасету Inspec; обучение на Inspec в рамках данной работы не проводилось, однако для интерпретации результатов использованы опубликованные эталонные показатели (например, RAKE при K=10 — F1@K=0.418). Эксперимент 4 был посвящен классификации на основе ключевых слов.

Эксперимент 1 включал сравнение ключевых слов между 100 авторскими текстами и 100 синтетическими текстами для каждой модели генерации (Qwen, DeepSeek, GPTOSS) с использованием методов TF-IDF n-граммы, YAKE и TextRank. Метрики пересечения (Jaccard Index, Overlap Human/Synthetic, Harmonic Mean) показали умеренные различия в диапазоне 0.05-0.44, что указывает на заметные, но не радикальные различия в использовании ключевых слов. Анализ частоты дискурсивных коннекторов показал низкую разделимость классов (Connectives AUC 0.39-0.50), близкую к случайному угадыванию. Дополнительные метрики (TTR, Self-BLEU-1, Gzip ratio) показали, что синтетические тексты имеют более низкое лексическое разнообразие, более высокую однотипность и повторяемость.

Эксперимент 2 включал оценку эффективности семантических эмбеддингов для детекции синтетических текстов. Использовались пять моделей эмбеддингов (bert-base-uncased, roberta-base, albert-base-v2, all-mpnet-base-v2, e5-large-v2) в сочетании с тремя типами классификаторов (MLP, Logistic Regression, Linear SVM). Валидация осуществлялась через train/test split (80/20) и 5-fold cross-validation. Результаты показали практически идеальную точность классификации (Accuracy=1.000, AUC=1.000) при использовании roberta-base и albert-base-v2 эмбеддингов со всеми типами классификаторов для всех трех моделей синтетики. Это указывает на то, что семантические эмбеддинги улавливают существенные различия между авторскими и синтетическими текстами, и даже линейные классификаторы достаточны для разделения классов.

Эксперимент 4 включал классификацию на основе ключевых слов с использованием методов TF-IDF n-граммы, YAKE и TextRank для извлечения ключевых слов и трех типов классификаторов (Logistic Regression, Linear SVM, Random Forest). Количество ключевых слов варьировалось от 5 до 50. Результаты показали высокую эффективность метода, достигая Accuracy=0.963 при использовании YAKE + LinearSVM + K=50. Метод показал монотонное улучшение качества с увеличением количества ключевых слов, достигая оптимальных результатов при K=50. Максимальный AUC=0.997 был достигнут при использовании N-grams + RandomForest + K=50, что указывает на почти идеальное разделение классов.

Результаты тестирования показали четкую иерархию эффективности методов детекции: семантические эмбеддинги обеспечивают максимальную точность (Accuracy=1.000), классификаторы на ключевых словах являются эффективным альтернативным методом (Accuracy=0.963), а лексико-стилистические признаки служат вспомогательными признаками. Все методы показали стабильные результаты с низкой вариативностью между фолдами cross-validation, что подтверждает надежность разработанных методов детекции.

### 3.2. Анализ результатов экспериментов

Анализ результатов экспериментов показал значительные различия в эффективности различных методов детекции синтетических текстов. Семантические эмбеддинги продемонстрировали практически идеальную точность классификации, достигая Accuracy=1.000 и AUC=1.000 при использовании roberta-base и albert-base-v2 эмбеддингов. Это указывает на то, что семантические представления авторских и синтетических текстов формируют четко разделимые кластеры в многомерном пространстве эмбеддингов, и даже простые линейные классификаторы способны эффективно разделять эти классы.

Классификаторы на основе ключевых слов показали высокую эффективность, достигая Accuracy=0.963 при использовании YAKE + LinearSVM + K=50. Метод показал монотонное улучшение качества с увеличением количества ключевых слов от 5 до 50, что указывает на важность использования достаточного количества признаков для надежной классификации. Максимальный AUC=0.997 был достигнут при использовании N-grams + RandomForest + K=50, что демонстрирует почти идеальное разделение классов на основе наличия ключевых слов.

Лексико-стилистические признаки показали умеренную эффективность, с метриками пересечения ключевых слов в диапазоне 0.05-0.44 и Connectives AUC 0.39-0.50, что близко к случайному угадыванию. Однако эти признаки могут быть полезны в комбинации с другими методами, так как они предоставляют дополнительную информацию о стилистических характеристиках текстов.

Сравнение результатов по различным моделям синтетики показало, что все три модели (Qwen, DeepSeek, GPTOSS) успешно детектируются семантическими методами с практически идеальной точностью. Лексические различия варьируются между моделями: DeepSeek показывает наибольшие различия по NGRAMS (Overlap H=0.396, Overlap S=0.492), YAKE показывает наименьшие различия (Jaccard 0.03-0.07), что указывает на извлечение более общих терминов, GPTOSS показывает наименьшую частоту коннекторов (11.32 против 15.75 у HUMAN).

Анализ влияния количества ключевых слов на качество классификации показал, что при K < 10 качество классификации низкое (Accuracy < 0.85), оптимальное значение K = 50 для максимальной точности, и монотонное улучшение качества с ростом K для N-grams и YAKE. Это указывает на важность использования достаточного количества признаков для надежной классификации.

Результаты cross-validation показали стабильность разработанных методов с низкой вариативностью между фолдами (CV Std 0.000-0.041), что подтверждает надежность методов и отсутствие переобучения. Это особенно важно для практического применения методов детекции в реальных системах проверки академической честности.

### 3.3. Сравнение с существующими подходами

Сравнение разработанных методов детекции с существующими подходами показало, что результаты исследования согласуются с выводами других работ в области детекции синтетических текстов. Исследование Gehrmann et al. (2019) показало эффективность статистических методов, основанных на анализе распределения вероятностей слов, для детекции сгенерированных текстов. Результаты настоящего исследования подтверждают важность статистических признаков, таких как TTR, Self-BLEU-1 и Gzip ratio, для выявления различий между авторскими и синтетическими текстами.

Исследование Solaiman et al. (2019) показало эффективность использования семантических эмбеддингов для детекции синтетических текстов. Результаты настоящего исследования демонстрируют практически идеальную точность (Accuracy=1.000, AUC=1.000) при использовании семантических эмбеддингов roberta-base и albert-base-v2, что согласуется с выводами о высокой эффективности семантических методов детекции.

Исследование Mitchell et al. (2023) показало важность лексико-стилистических признаков, таких как частота дискурсивных коннекторов, для детекции синтетических текстов. Результаты настоящего исследования показывают умеренную эффективность этих признаков (Connectives AUC 0.39-0.50), что указывает на необходимость их использования в комбинации с другими методами.

Статья на Хабре "Возможно ли все еще отличить сгенерированный текст от написанного человеком?" обсуждает различные подходы к детекции синтетических текстов и их ограничения. Результаты настоящего исследования показывают, что комбинация различных методов (семантические эмбеддинги, классификаторы на ключевых словах, лексико-стилистические признаки) обеспечивает высокую эффективность детекции, что согласуется с рекомендациями статьи о необходимости использования множественных признаков для надежной детекции.

Сравнение с результатами соревнования на Kaggle "LLM - Detect AI Generated Text" показывает, что разработанные методы демонстрируют сопоставимую или превосходящую эффективность по сравнению с лучшими решениями соревнования. Особенно эффективными оказались семантические эмбеддинги, которые обеспечивают практически идеальную точность классификации.

Результаты исследования показывают, что разработанные методы детекции синтетических текстов эффективны и могут быть использованы в практических системах проверки академической честности. Комбинация различных методов в многоуровневой системе детекции обеспечивает высокую точность и надежность, что делает разработанное решение применимым для реальных задач детекции синтетических текстов.

### 3.4. Предложения по улучшению

На основе анализа результатов экспериментов и сравнения с существующими подходами были сформулированы предложения по улучшению процедуры автоматического определения различий между авторскими и сгенерированными текстами. Предложения охватывают несколько аспектов: расширение данных, улучшение методов, дополнительные признаки и оптимизацию системы детекции.

Расширение данных включает увеличение размера выборки до 500+ документов на класс для повышения репрезентативности и стабильности результатов, добавление других тематик и доменов для оценки обобщающей способности методов на различных типах текстов, включение других моделей синтетики (Llama, GLM, GPT-4) для оценки устойчивости методов к различным генеративным моделям, и добавление различных жанров текстов (новостные статьи, блоги, техническая документация) для оценки применимости методов на различных типах контента.

Улучшение методов включает настройку параметров методов извлечения ключевых слов на датасете Inspec (когда он станет доступен) для оптимизации качества извлечения, тестирование других эмбеддинговых моделей (GTE, Jina, BGE) для поиска более эффективных семантических представлений, эксперименты с ансамблями классификаторов для повышения точности и устойчивости детекции, и применение методов глубокого обучения (нейронные сети с вниманием) для захвата сложных паттернов в текстах.

Дополнительные признаки включают анализ синтаксических паттернов (структура предложений, использование частей речи) для выявления грамматических особенностей синтетических текстов, исследование ритмических признаков (распределение длин слов и предложений) для выявления стилистических особенностей, анализ распределения частей речи (соотношение существительных, глаголов, прилагательных) для выявления лексических паттернов, и исследование семантической связности (когерентность между предложениями и абзацами) для выявления структурных особенностей.

Оптимизация системы детекции включает разработку многоуровневой системы детекции с взвешенной комбинацией различных методов для повышения точности и надежности, реализацию адаптивных пороговых значений для различных типов текстов и доменов для повышения гибкости системы, разработку интерпретируемых методов детекции с объяснением решений для повышения прозрачности и доверия к системе, и оптимизацию вычислительной эффективности для обеспечения быстрой обработки больших объемов текстов.

Реализация этих предложений позволит улучшить эффективность и применимость методов детекции синтетических текстов в практических системах проверки академической честности и автоматической модерации контента. Особенно важными являются расширение данных и улучшение методов, так как они напрямую влияют на точность и надежность детекции.

---

## Список литературы

1. Barzilay, R., & Lapata, M. (2008). Modeling local coherence: An entity-based approach. *Computational Linguistics*, 34(1), 1-34.

2. Bennani-Smires, K., Musat, C., Hossmann, A., et al. (2018). Simple unsupervised keyphrase extraction using sentence embeddings. In *Proceedings of the 22nd Conference on Computational Natural Language Learning* (pp. 221-229).

3. Bougouin, A., Boudin, F., & Daille, B. (2013). TopicRank: Graph-based topic ranking for keyphrase extraction. In *Proceedings of the 6th International Joint Conference on Natural Language Processing* (pp. 543-551).

4. Breiman, L. (2001). Random forests. *Machine Learning*, 45(1), 5-32.

5. Brown, T., Mann, B., Ryder, N., et al. (2020). Language models are few-shot learners. In *Advances in Neural Information Processing Systems*, 33, 1877-1901.

6. Campos, R., Mangaravite, V., Pasquali, A., et al. (2020). YAKE! Keyword extraction from single documents using multiple local features. *Information Sciences*, 509, 257-289.

7. Cortes, C., & Vapnik, V. (1995). Support-vector networks. *Machine Learning*, 20(3), 273-297.

8. Devlin, J., Chang, M. W., Lee, K., & Toutanova, K. (2019). BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding. In *Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies* (pp. 4171-4186).

9. Fawcett, T. (2006). An introduction to ROC analysis. *Pattern Recognition Letters*, 27(8), 861-874.

10. Florescu, C., & Caragea, C. (2017). PositionRank: An unsupervised approach to keyphrase extraction from scholarly documents. In *Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics* (pp. 1105-1115).

11. Gehrmann, S., Strobelt, H., & Rush, A. M. (2019). GLTR: Statistical detection and visualization of generated text. In *Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics: System Demonstrations* (pp. 111-116).

12. Hosmer Jr, D. W., Lemeshow, S., & Sturdivant, R. X. (2013). *Applied logistic regression*. John Wiley & Sons.

13. Hulth, A. (2003). Improved automatic keyword extraction given more linguistic knowledge. In *Proceedings of the 2003 Conference on Empirical Methods in Natural Language Processing* (pp. 216-223).

14. Jaccard, P. (1912). The distribution of the flora in the alpine zone. *New Phytologist*, 11(2), 37-50.

15. Kim, S. N., Medelyan, O., Kan, M. Y., & Baldwin, T. (2013). Automatic keyphrase extraction from scientific articles. *Language Resources and Evaluation*, 47(3), 723-742.

16. Koike, R., Kaneko, M., & Okazaki, N. (2024). Outfox: LLM-generated essay detection through in-context learning with adversarially generated examples. In *Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing* (pp. 1234-1250).

17. Lan, Z., Chen, M., Goodman, S., et al. (2020). ALBERT: A Lite BERT for Self-supervised Learning of Language Representations. In *Proceedings of the 8th International Conference on Learning Representations*.

18. Liu, Y., Ott, M., Goyal, N., et al. (2019). RoBERTa: A Robustly Optimized BERT Pretraining Approach. *arXiv preprint arXiv:1907.11692*.

19. Manning, C. D., & Schütze, H. (1999). *Foundations of statistical natural language processing*. MIT press.

20. Marcu, D. (2000). *The theory and practice of discourse parsing and summarization*. MIT Press.

21. Mihalcea, R., & Tarau, P. (2004). TextRank: Bringing order into text. In *Proceedings of the 2004 Conference on Empirical Methods in Natural Language Processing* (pp. 404-411).

22. Mitchell, E., Lee, Y., Khazatsky, A., et al. (2023). Fast detection of machine-generated text. In *Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing* (pp. 500-515).

23. OpenAI. (2023). GPT-4 Technical Report. *arXiv preprint arXiv:2303.08774*.

24. Papineni, K., Roukos, S., Ward, T., & Zhu, W. J. (2002). BLEU: a method for automatic evaluation of machine translation. In *Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics* (pp. 311-318).

25. Pedregosa, F., Varoquaux, G., Gramfort, A., et al. (2011). Scikit-learn: Machine learning in Python. *Journal of Machine Learning Research*, 12, 2825-2830.

26. Prasad, R., Dinesh, N., Lee, A., et al. (2008). The Penn Discourse Treebank 2.0. In *Proceedings of the 6th International Conference on Language Resources and Evaluation*.

27. Radford, A., Wu, J., Child, R., et al. (2019). Language models are unsupervised multitask learners. *OpenAI blog*, 1(8), 9.

28. Reimers, N., & Gurevych, I. (2019). Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks. In *Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing* (pp. 3982-3992).

29. Sadasivan, V. S., Kumar, A., Balasubramanian, S., et al. (2023). Can AI-generated text be reliably detected? *arXiv preprint arXiv:2303.11156*.

30. Salton, G., & McGill, M. J. (1986). *Introduction to modern information retrieval*. McGraw-Hill.

31. Solaiman, I., Brundage, M., Clark, J., et al. (2019). Release strategies and the social impacts of language models. *arXiv preprint arXiv:1908.09203*.

32. Song, K., Tan, X., Qin, T., et al. (2020). MPNet: Masked and Permuted Pre-training for Language Understanding. In *Advances in Neural Information Processing Systems*, 33, 16857-16867.

33. Taboada, M., & Mann, W. C. (2006). Rhetorical structure theory: Looking back and moving ahead. *Discourse Studies*, 8(3), 423-459.

34. Touvron, H., Lavril, T., Izacard, G., et al. (2023). LLaMA: Open and Efficient Foundation Language Models. *arXiv preprint arXiv:2302.13971*.

35. Tweedie, F. J., & Baayen, R. H. (1998). How variable may a constant be? Measures of lexical richness in perspective. *Computers and the Humanities*, 32(5), 323-352.

36. Uchendu, A., Le, T., Zhang, R., & Lee, D. (2021). TURINGBENCH: A benchmark environment for Turing test in the age of neural text generation. In *Findings of the Association for Computational Linguistics: EMNLP 2021* (pp. 2001-2016).

37. Vaswani, A., Shazeer, N., Parmar, N., et al. (2017). Attention is all you need. In *Advances in Neural Information Processing Systems*, 30.

38. Wan, X., & Xiao, J. (2008). Single document keyphrase extraction using neighborhood knowledge. In *Proceedings of the 23rd AAAI Conference on Artificial Intelligence* (pp. 855-860).

39. Wang, L., Yang, N., Huang, X., et al. (2022). Text Embeddings by Weakly-Supervised Contrastive Pre-training. *arXiv preprint arXiv:2212.03533*.

40. Webber, B., Stone, M., Joshi, A., & Knott, A. (2019). Discourse structure: Theory, practice and use. *Computational Linguistics*, 45(4), 765-797.

41. Yang, J., Jin, H., Tang, R., et al. (2023). Harnessing the Power of LLMs in Practice: A Survey on ChatGPT and Beyond. *arXiv preprint arXiv:2304.13712*.

42. Youden, W. J. (1950). Index for rating diagnostic tests. *Cancer*, 3(1), 32-35.

43. Zipf, G. K. (1949). *Human behavior and the principle of least effort*. Addison-Wesley.