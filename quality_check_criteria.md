# Критерии проверки качества сгенерированных текстов

## Описание процесса проверки

На третьем этапе проводилась проверка качества сгенерированных текстов. Проверка включала автоматическую фильтрацию по техническим критериям и оценку соответствия тематике, грамматической корректности и связности текста. Тексты, которые не соответствуют критериям качества, были заменены новыми генерациями.

## Автоматические критерии фильтрации

### 1. Минимальная длина текста
- **Метрика**: Количество символов в тексте (после удаления пробелов в начале и конце)
- **Критерий**: `len(text.strip()) >= min_chars`
- **Пороговое значение**: `min_chars = 300` символов (по умолчанию)
- **Назначение**: Исключение слишком коротких текстов, которые не соответствуют требованиям к объему научной аннотации

### 2. Отсутствие сообщений об ошибках API
- **Метрика**: Проверка наличия маркеров ошибок в тексте
- **Критерий**: Текст не должен начинаться с `[Generation error` или содержать следующие маркеры:
  - "Task 'text-generation' not supported"
  - "Available tasks:"
  - "fireworks-ai"
  - "Error:"
  - "[Generation error"
  - "HTTPException"
  - "Traceback (most recent call last)"
- **Назначение**: Исключение текстов, содержащих сообщения об ошибках API вместо сгенерированного контента

### 3. Отсутствие кода и структурированных данных
- **Метрика**: Проверка наличия паттернов, характерных для кода или JSON
- **Критерий**: Текст не должен содержать следующие паттерны:
  - `\bdef\b` (определения функций Python)
  - `\bclass\b` (определения классов)
  - `import ` (импорты)
  - `\{\s*\"` (начало JSON объекта)
  - `\}\s*$` (конец JSON объекта)
- **Назначение**: Исключение текстов, содержащих код или структурированные данные вместо естественного текста

### 4. Санитизация вывода
- **Метрика**: Удаление служебных блоков и метаданных
- **Процесс**: 
  - Удаление блоков размышлений (````thinking...````, `<think>...</think>`)
  - Удаление кодовых блоков (````...````)
  - Удаление HTML-тегов (`<code>`, `<pre>`, `<details>`, `<summary>`)
  - Удаление префиксов типа "Final abstract:", "Abstract:"
  - Удаление строк, начинающихся с "analysis:", "reasoning:", "thought:", "chain-of-thought:"
- **Назначение**: Очистка текста от служебной информации, добавленной моделью в процессе генерации

## Качественные критерии оценки

### 1. Соответствие тематике
- **Метод оценки**: Сравнение сгенерированного текста с исходным промптом и заголовком статьи
- **Критерии**:
  - Использование терминологии, соответствующей тематике (Text Mining или Information Retrieval)
  - Соответствие содержания заданной теме исследования
  - Наличие релевантных технических деталей
- **Назначение**: Обеспечение релевантности сгенерированных текстов исходным тематикам

### 2. Грамматическая корректность
- **Метод оценки**: Визуальная проверка текста на наличие грамматических ошибок
- **Критерии**:
  - Правильное использование грамматических конструкций
  - Отсутствие явных грамматических ошибок
  - Соответствие академическому стилю письма
- **Назначение**: Обеспечение качества текста на уровне грамматики

### 3. Связность текста
- **Метод оценки**: Оценка логической связности и структуры текста
- **Критерии**:
  - Логическая последовательность изложения
  - Наличие связующих элементов между предложениями
  - Целостность и завершенность текста
  - Отсутствие обрывов или незавершенных мыслей
- **Назначение**: Обеспечение связности и читаемости текста

## Процесс проверки

1. **Автоматическая фильтрация**: Каждый сгенерированный текст проходит автоматическую проверку по техническим критериям (длина, отсутствие ошибок, отсутствие кода).

2. **Санитизация**: Текст очищается от служебных блоков и метаданных.

3. **Качественная оценка**: Тексты, прошедшие автоматическую фильтрацию, оцениваются по качественным критериям (соответствие теме, грамматика, связность).

4. **Замена некачественных текстов**: Тексты, не соответствующие критериям качества, исключаются из корпуса и заменяются новыми генерациями.

## Результат проверки

Результатом генерации стала коллекция из 300 документов, равномерно распределенных по 3-м моделям генерации (Qwen, DeepSeek, GPT-OSS) и 2-м темам (Text Mining, Information Retrieval), все из которых соответствуют установленным критериям качества.


## Метрики самопохожести и разнообразия

### Self-BLEU-1 (вариант для корпуса)
- **Назначение**: измеряет самопохожесть корпуса; более высокий Self‑BLEU‑1 означает меньшее разнообразие между документами.
- **Гранулярность**: считаем для каждого документа относительно остальных, затем усредняем по корпусу.
- **Токенизация и нормализация**:
  - Базовый вариант: точные совпадения токенов (словоформ), без стемминга/лемматизации.
  - Рекомендуемый для анализа различий HUMAN vs AI: лемматизация и удаление стоп‑слов на едином пайплайне предобработки, чтобы снизить влияние флексии и общеязыковых слов.
  - Язык‑зависимость: использовать один и тот же токенайзер и лемматизатор для всех документов.

#### Определение
- Пусть \(C = \{d_1,\dots,d_N\}\) — корпус документов (после выбранной нормализации).
- Объединим все документы, кроме \(d_i\), в множество «эталонов» \(R_i = \{d_j : j \neq i\}\).
- Обозначим модифицированную точность униграмм (с клиппингом частот) как \(P_1(d_i, R_i)\).

\[
\mathrm{Self\text{-}BLEU\text{-}1}(C) \;=\; \frac{1}{N}\sum_{i=1}^{N} P_1\!\big(d_i, R_i\big)
\]

- Примечания:
  - Используем только униграммную компоненту (без brevity penalty и без сглаживания).
  - Для варианта «по словоформам» и «по леммам» считаем отдельно — это два разных признака.

#### Протокол вычисления
- **Нормализация**:
  - Вариант A (словоформы): нижний регистр, удаление пунктуации.
  - Вариант B (леммы): нижний регистр, удаление пунктуации, лемматизация, удаление стоп‑слов.
- **Подсчет**:
  - Для каждого \(d_i\) считаем \(P_1\) относительно объединённого пула референсов \(R_i\) (все остальные документы).
  - Усредняем по всем \(i\): получаем среднее и стандартное отклонение.
- **Отчёт**: приводим mean±std по корпусам HUMAN и AI; дополнительно можно показывать распределения (ящики/гистограммы).

#### Интерпретация
- **Выше значение** → более однотипные тексты, меньше разнообразия.
- **Ниже значение** → тексты разнообразнее по лексике.
- Рекомендуется дополнять другими метриками разнообразия (см. ниже) и проводить сравнение распределений между HUMAN и AI.
 - Это не метрика «качества генерации» относительно внешнего эталона; она оценивает внутреннее лексическое разнообразие корпуса.
 - В рамках данного проекта основным отчётным значением является Self‑BLEU‑1 (lemma); вариант по словоформам приводится дополнительно для полноты.

### Дополнительные метрики разнообразия
- **distinct‑1 / distinct‑2**: доля уникальных униграмм/биграмм в корпусе.
- **TTR (Type‑Token Ratio)**: отношение числа типов к числу токенов.
- **Zipf slope**: наклон лог‑лог регрессии частот по рангу; отражает распределение лексики.
- **Gzip ratio**: степень сжатия текста; выше при большей повторяемости.

### Рекомендации по использованию
- Считать Self‑BLEU‑1 в двух вариантах: по словоформам и по леммам (без стоп‑слов).
- Сопровождать Self‑BLEU отчётом distinct‑1/2, TTR, Zipf slope, Gzip ratio.
- Для корректного сравнения корпусов использовать единый пайплайн нормализации и единую токенизацию.

### Connectives AUC (по частоте дискурсивных коннекторов)
- **Назначение**: оценивает разделимость HUMAN и AI по частоте дискурсивных коннекторов.
- **Позитивный класс**: AI.
- **Список коннекторов**: фиксированный словарь (например, however, therefore, moreover, furthermore, in addition, in contrast, notably и др.), сопоставляется по границам слов.

#### Признак
Пусть \(d\) — документ, \(W(d)\) — число слов, \(\mathcal{C}=\{c_1,\dots,c_M\}\) — словарь коннекторов, а \(\mathrm{cnt}(c_i,d)\) — число вхождений \(c_i\) в \(d\) (с учётом границ слова). Тогда
\[
\mathrm{rate}(d) \;=\; \frac{\sum_{i=1}^{M}\mathrm{cnt}(c_i,d)}{W(d)} \times 1000.
\]
Скор балла для ROC считаем как \(s(d)=\mathrm{rate}(d)\).

Обозначим метку класса \(y(d)\in\{0,1\}\) (0=HUMAN, 1=AI). Для всех документов строим ROC‑кривую по порогу \(\tau\) на \(s(d)\):
\[
\mathrm{TPR}(\tau)=\frac{\mathrm{TP}(\tau)}{\mathrm{TP}(\tau)+\mathrm{FN}(\tau)},\quad
\mathrm{FPR}(\tau)=\frac{\mathrm{FP}(\tau)}{\mathrm{FP}(\tau)+\mathrm{TN}(\tau)}.
\]

#### Формула AUC
- Непрерывная форма:
\[
\mathrm{AUC}=\int_{0}^{1} \mathrm{TPR}\big(\mathrm{FPR}^{-1}(x)\big)\,dx.
\]
- Дискретная (трапециевидная) аппроксимация по узлам \((\mathrm{FPR}_i,\mathrm{TPR}_i)\):
\[
\mathrm{AUC}\approx \sum_{i=1}^{K-1} \left(\mathrm{FPR}_{i+1}-\mathrm{FPR}_{i}\right)\cdot \frac{\mathrm{TPR}_{i+1}+\mathrm{TPR}_{i}}{2}.
\]
- Эквивалентная интерпретация Манна–Уитни:
\[
\mathrm{AUC}=\mathbb{P}\!\big(s(d_{\mathrm{AI}}) > s(d_{\mathrm{HUM}})\big)+\tfrac{1}{2}\mathbb{P}\!\big(s(d_{\mathrm{AI}})=s(d_{\mathrm{HUM}})\big).
\]

#### Интерпретация
- **AUC≈0.5**: случайное ранжирование (признак не разделяет классы).
- **0.5–0.7**: слабая разделимость; полезно как компонент ансамбля.
- **0.7–0.9**: умеренная разделимость.
- **≥0.9**: высокая разделимость (редко для одного признака).
- Если AI имеет меньшие значения \(\mathrm{rate}\), можно инвертировать скор \(s'(d)=-s(d)\) — AUC станет >0.5.

#### Обозначения
- \(d\) — документ; \(W(d)\) — число слов; \(\mathcal{C}\) — словарь коннекторов; \(\mathrm{cnt}(c_i,d)\) — число вхождений коннектора \(c_i\).
- \(s(d)\) — скор; \(y(d)\) — метка (0=HUMAN, 1=AI); TPR, FPR — характеристики ROC.

### Индекс Симпсона (лексическое разнообразие)
- **Назначение**: оценивает вероятность того, что два случайно выбранных токена будут одного и того же типа; в форме \(1-\sum p_i^2\) — мера разнообразия.

Пусть \(V(d)=\{t_1,\dots,t_K\}\) — множество типов в документе \(d\), \(n_i\) — частота \(t_i\), \(N=\sum_i n_i\), \(p_i=\frac{n_i}{N}\).

#### Формулы
- Оценка через относительные частоты:
\[
D_{\mathrm{Simpson}}(d) \;=\; 1 - \sum_{i=1}^{K} p_i^2.
\]
- Несмещённая выборочная форма:
\[
D_{\mathrm{Simpson}}(d) \;=\; 1 - \frac{\sum_{i=1}^{K} n_i(n_i-1)}{N(N-1)}.
\]

#### Интерпретация
- Ближе к 1 → больше разнообразие лексики; ближе к 0 → сильная повторяемость.
- Рекомендуется сравнивать распределения \(D_{\mathrm{Simpson}}\) между классами и дополнять TTR/Zipf slope.

#### Обозначения
- \(t_i\) — тип слова; \(n_i\) — его частота; \(N\) — число токенов; \(p_i\) — доля типа.

### Sentiment Score (тональность текста)
- **Назначение**: характеризует эмоциональную окраску текста; используется для сравнения профилей HUMAN и AI.
- **Инструмент**: VADER (NLTK), возвращает компоненты \( \mathrm{pos},\mathrm{neg},\mathrm{neu}\in[0,1]\) и \(\mathrm{compound}\in[-1,1]\).

#### Документный скор
Основной показатель: \(\mathrm{compound}(d)\) — нормализованная взвешенная сумма лексиконных оценок с модификаторами:
\[
\mathrm{compound}(d)=\mathrm{normalize}\!\left(\sum_{w\in d} \mathrm{lex}(w)\cdot \alpha(w,d)\right),\quad \mathrm{compound}\in[-1,1],
\]
где \(\mathrm{lex}(w)\) — лексиконная полярность слова \(w\), \(\alpha(w,d)\) — совокупный модификатор (усилители, отрицания, пунктуация, регистр), normalize — приводит сумму к \([-1,1]\).

#### Агрегация по корпусу
\[
\overline{\mathrm{compound}}(C)=\frac{1}{|C|}\sum_{d\in C}\mathrm{compound}(d),\quad
\Delta_{\mathrm{compound}}=\overline{\mathrm{compound}}(C_{\mathrm{AI}})-\overline{\mathrm{compound}}(C_{\mathrm{HUM}}).
\]

#### Интерпретация
- \(\mathrm{compound}\approx 0\) — нейтральная тональность; >0 — позитивная; <0 — негативная.
- Для детекции сравнивают распределения \(\mathrm{compound}\) и/или \(\Delta_{\mathrm{compound}}\) между HUMAN и AI; различия невелики в научном домене, поэтому метрика вспомогательная.

#### Обозначения
- \(d\) — документ; \(w\) — слово; \(C\) — корпус; \(|C|\) — число документов; \(\mathrm{lex}(w)\) — лексиконная оценка; \(\alpha(w,d)\) — модификатор контекста.

